// ğŸµ WAVæ ¼å¼æµå¼è½¬å½•æœåŠ¡ - å¼ºåˆ¶ä½¿ç”¨WAVæ ¼å¼ç¡®ä¿Whisperå…¼å®¹æ€§
// ä¸“é—¨è§£å†³éŸ³é¢‘æ ¼å¼å…¼å®¹é—®é¢˜

interface WAVStreamingConfig {
  chunkInterval: number;
  translationDelay: number;
}

type WAVEventType = 'transcription_update' | 'translation_update' | 'error';

interface WAVEvent {
  type: WAVEventType;
  data: any;
  timestamp: number;
}

export class WAVStreamingTranscriptionService {
  private config: WAVStreamingConfig;
  private mediaRecorder: MediaRecorder | null = null;
  private audioStream: MediaStream | null = null;
  private audioContext: AudioContext | null = null;
  
  // ç®€åŒ–çŠ¶æ€
  private currentText = '';
  private isRecording = false;
  private audioChunks: Float32Array[] = [];
  
  // äº‹ä»¶ç³»ç»Ÿ
  private eventListeners: Map<WAVEventType, Set<(event: WAVEvent) => void>> = new Map();
  
  // å®šæ—¶å™¨
  private recordTimer: NodeJS.Timeout | null = null;
  private translationTimer: NodeJS.Timeout | null = null;

  constructor(config: Partial<WAVStreamingConfig> = {}) {
    this.config = {
      chunkInterval: 3000,
      translationDelay: 1000,
      ...config
    };
    
    this.initializeEventMaps();
  }

  private initializeEventMaps() {
    const eventTypes: WAVEventType[] = ['transcription_update', 'translation_update', 'error'];
    eventTypes.forEach(type => {
      this.eventListeners.set(type, new Set());
    });
  }

  // ğŸ¤ å¼€å§‹å½•éŸ³ - ä½¿ç”¨AudioContextç›´æ¥å½•åˆ¶PCMæ•°æ®
  async startStreaming(): Promise<void> {
    try {
      console.log('ğŸµ å¯åŠ¨WAVæ ¼å¼æµå¼è½¬å½•æœåŠ¡');
      
      // è·å–éŸ³é¢‘æµ
      this.audioStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,      // Whisperæ¨èé‡‡æ ·ç‡
          channelCount: 1,        // å•å£°é“
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      // åˆå§‹åŒ–AudioContextè¿›è¡ŒPCMå½•åˆ¶
      await this.initializeAudioContext();
      
      this.isRecording = true;
      this.startRecordingLoop();
      
      console.log('âœ… WAVæ ¼å¼æµå¼è½¬å½•æœåŠ¡å¯åŠ¨æˆåŠŸ');
    } catch (error) {
      this.emitEvent('error', { error, message: 'å¯åŠ¨WAVæµå¼è½¬å½•å¤±è´¥' });
      throw error;
    }
  }

  // ğŸ§ åˆå§‹åŒ–AudioContext - ç›´æ¥å½•åˆ¶PCMæ•°æ®
  private async initializeAudioContext(): Promise<void> {
    this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
      sampleRate: 16000  // å¼ºåˆ¶16kHzé‡‡æ ·ç‡
    });
    
    const source = this.audioContext.createMediaStreamSource(this.audioStream!);
    const processor = this.audioContext.createScriptProcessor(4096, 1, 1);
    
    // æ”¶é›†PCMéŸ³é¢‘æ•°æ®
    processor.onaudioprocess = (event) => {
      const inputBuffer = event.inputBuffer;
      const inputData = inputBuffer.getChannelData(0);
      
      // å¤åˆ¶éŸ³é¢‘æ•°æ®
      const audioData = new Float32Array(inputData.length);
      audioData.set(inputData);
      this.audioChunks.push(audioData);
    };
    
    source.connect(processor);
    processor.connect(this.audioContext.destination);
    
    console.log('ğŸ§ AudioContextåˆå§‹åŒ–å®Œæˆï¼Œé‡‡æ ·ç‡:', this.audioContext.sampleRate);
  }

  // ğŸ”„ å®šæ—¶å½•éŸ³å¾ªç¯
  private startRecordingLoop(): void {
    this.recordTimer = setInterval(async () => {
      try {
        await this.processAudioChunks();
      } catch (error) {
        console.error('âŒ WAVå½•éŸ³å¾ªç¯é”™è¯¯:', error);
        this.emitEvent('error', { error, message: 'WAVå½•éŸ³å¤„ç†é”™è¯¯' });
      }
    }, this.config.chunkInterval);
  }

  // ğŸ“ å¤„ç†éŸ³é¢‘å— - è½¬æ¢ä¸ºWAVæ ¼å¼
  private async processAudioChunks(): Promise<void> {
    if (this.audioChunks.length === 0) {
      console.log('âš ï¸ æ²¡æœ‰PCMéŸ³é¢‘æ•°æ®å¯å¤„ç†');
      return;
    }

    try {
      // åˆå¹¶PCMæ•°æ®
      const pcmData = this.mergePCMChunks();
      
      // è½¬æ¢ä¸ºWAVæ ¼å¼
      const wavBlob = this.createWAVBlob(pcmData, 16000, 1);
      
      // é‡ç½®éŸ³é¢‘å—
      this.audioChunks = [];
      
      console.log('ğŸµ WAVéŸ³é¢‘æ•°æ®å‡†å¤‡å®Œæˆ:', {
        size: wavBlob.size,
        type: wavBlob.type,
        sampleCount: pcmData.length
      });

      // è½¬å½•éŸ³é¢‘
      const transcriptionResult = await this.transcribeAudio(wavBlob);
      
      if (transcriptionResult.text && transcriptionResult.text.trim()) {
        const newText = transcriptionResult.text.trim();
        this.currentText = this.currentText ? 
          this.currentText + ' ' + newText : newText;
        
        console.log('ğŸ“ WAVè½¬å½•æ›´æ–°:', this.currentText);
        
        this.emitEvent('transcription_update', {
          text: this.currentText,
          confidence: transcriptionResult.confidence || 0.9,
          timestamp: Date.now()
        });
        
        this.scheduleTranslation();
      }
      
    } catch (error) {
      console.error('âŒ WAVéŸ³é¢‘å¤„ç†å¤±è´¥:', error);
      this.emitEvent('error', { 
        error, 
        message: `WAVå¤„ç†å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}` 
      });
    }
  }

  // ğŸ”€ åˆå¹¶PCMæ•°æ®
  private mergePCMChunks(): Float32Array {
    if (this.audioChunks.length === 0) {
      throw new Error('æ²¡æœ‰PCMæ•°æ®å¯åˆå¹¶');
    }

    // è®¡ç®—æ€»é•¿åº¦
    const totalLength = this.audioChunks.reduce((sum, chunk) => sum + chunk.length, 0);
    
    // åˆ›å»ºåˆå¹¶æ•°ç»„
    const mergedData = new Float32Array(totalLength);
    let offset = 0;
    
    for (const chunk of this.audioChunks) {
      mergedData.set(chunk, offset);
      offset += chunk.length;
    }
    
    console.log('ğŸ”€ PCMæ•°æ®åˆå¹¶å®Œæˆ:', {
      chunks: this.audioChunks.length,
      totalSamples: totalLength,
      durationSeconds: totalLength / 16000
    });
    
    return mergedData;
  }

  // ğŸµ åˆ›å»ºWAVæ ¼å¼Blob
  private createWAVBlob(pcmData: Float32Array, sampleRate: number, channels: number): Blob {
    const length = pcmData.length;
    const buffer = new ArrayBuffer(44 + length * 2);
    const view = new DataView(buffer);
    
    // WAVæ–‡ä»¶å¤´
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };
    
    // RIFF chunk descriptor
    writeString(0, 'RIFF');
    view.setUint32(4, 36 + length * 2, true);  // ChunkSize
    writeString(8, 'WAVE');
    
    // fmt sub-chunk
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);              // Subchunk1Size (PCM)
    view.setUint16(20, 1, true);               // AudioFormat (PCM)
    view.setUint16(22, channels, true);        // NumChannels
    view.setUint32(24, sampleRate, true);      // SampleRate
    view.setUint32(28, sampleRate * channels * 2, true); // ByteRate
    view.setUint16(32, channels * 2, true);    // BlockAlign
    view.setUint16(34, 16, true);              // BitsPerSample
    
    // data sub-chunk
    writeString(36, 'data');
    view.setUint32(40, length * 2, true);      // Subchunk2Size
    
    // å†™å…¥PCMæ•°æ® (è½¬æ¢Float32åˆ°Int16)
    let offset = 44;
    for (let i = 0; i < length; i++) {
      const sample = Math.max(-1, Math.min(1, pcmData[i]));
      const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
      view.setInt16(offset, intSample, true);
      offset += 2;
    }
    
    return new Blob([buffer], { type: 'audio/wav' });
  }

  // ğŸ—£ï¸ è½¬å½•éŸ³é¢‘
  private async transcribeAudio(audioBlob: Blob): Promise<any> {
    const { getAudioService } = await import('@/services');
    const audioService = getAudioService();
    
    console.log('ğŸ—£ï¸ å¼€å§‹WAVéŸ³é¢‘è½¬å½•:', {
      size: audioBlob.size,
      type: audioBlob.type,
      isWAV: audioBlob.type === 'audio/wav'
    });
    
    return await audioService.transcribe(audioBlob, {
      language: 'en',
      temperature: 0.2
    });
  }

  // ğŸŒ è°ƒåº¦ç¿»è¯‘
  private scheduleTranslation(): void {
    if (this.translationTimer) {
      clearTimeout(this.translationTimer);
    }
    
    this.translationTimer = setTimeout(async () => {
      if (this.currentText && this.currentText.length > 3) {
        await this.translateText();
      }
    }, this.config.translationDelay);
  }

  // ğŸŒ ç¿»è¯‘æ–‡æœ¬
  private async translateText(): Promise<void> {
    try {
      const { getTranslationService } = await import('@/services');
      const translationService = getTranslationService();
      
      console.log('ğŸŒ å¼€å§‹ç¿»è¯‘WAVè½¬å½•ç»“æœ:', this.currentText.substring(0, 50) + '...');
      
      const result = await translationService.translate(
        this.currentText,
        'en',
        'zh'
      );
      
      console.log('ğŸŒ WAVè½¬å½•ç¿»è¯‘å®Œæˆ:', result.translatedText);
      
      this.emitEvent('translation_update', {
        text: this.currentText,
        translation: result.translatedText,
        timestamp: Date.now()
      });
      
    } catch (error) {
      console.error('âŒ WAVè½¬å½•ç¿»è¯‘å¤±è´¥:', error);
      this.emitEvent('error', { 
        error, 
        message: `ç¿»è¯‘å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}` 
      });
    }
  }

  // ğŸ›‘ åœæ­¢æœåŠ¡
  async stopStreaming(): Promise<void> {
    console.log('ğŸ›‘ åœæ­¢WAVæµå¼è½¬å½•æœåŠ¡');
    
    this.isRecording = false;
    
    // æ¸…ç†å®šæ—¶å™¨
    if (this.recordTimer) {
      clearInterval(this.recordTimer);
      this.recordTimer = null;
    }
    
    if (this.translationTimer) {
      clearTimeout(this.translationTimer);
      this.translationTimer = null;
    }
    
    // æ¸…ç†éŸ³é¢‘èµ„æº
    if (this.audioContext) {
      await this.audioContext.close();
      this.audioContext = null;
    }
    
    if (this.audioStream) {
      this.audioStream.getTracks().forEach(track => track.stop());
      this.audioStream = null;
    }
    
    // é‡ç½®çŠ¶æ€
    this.currentText = '';
    this.audioChunks = [];
    
    console.log('âœ… WAVæµå¼è½¬å½•æœåŠ¡å·²åœæ­¢');
  }

  // ğŸ“¡ äº‹ä»¶ç³»ç»Ÿ
  addEventListener(type: WAVEventType, listener: (event: WAVEvent) => void): void {
    const listeners = this.eventListeners.get(type);
    if (listeners) {
      listeners.add(listener);
    }
  }

  removeEventListener(type: WAVEventType, listener: (event: WAVEvent) => void): void {
    const listeners = this.eventListeners.get(type);
    if (listeners) {
      listeners.delete(listener);
    }
  }

  private emitEvent(type: WAVEventType, data: any): void {
    const event: WAVEvent = {
      type,
      data,
      timestamp: Date.now()
    };
    
    const listeners = this.eventListeners.get(type);
    if (listeners) {
      listeners.forEach(listener => {
        try {
          listener(event);
        } catch (error) {
          console.error(`âŒ WAVäº‹ä»¶ç›‘å¬å™¨é”™è¯¯ [${type}]:`, error);
        }
      });
    }
  }

  // ğŸ“ˆ è·å–çŠ¶æ€
  getStatus() {
    return {
      isRecording: this.isRecording,
      currentText: this.currentText,
      audioChunksCount: this.audioChunks.length,
      config: this.config
    };
  }
}